#N canvas 900 67 1323 810 12;
#X msg 210 432 device cpu;
#X msg 189 381 add_zero_attn \$1;
#X obj 189 359 tgl 19 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000 0 1;
#X msg 169 328 bias \$1;
#X obj 169 303 tgl 19 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000 0 1;
#X msg 101 85 embed 4;
#X msg 121 150 heads 2;
#X msg 66 44 1 2 3 4 5 6 7 8 9 10 11 12;
#X text 853 99 destroy the current module and create new;
#X msg 777 136 info;
#X msg 790 175 clear;
#X text 820 135 list all activated module;
#X text 842 171 reset the current module (destroy and re-create a new module with the same name);
#X msg 766 100 set module1;
#X msg 213 476 add module1;
#X text 263 43 flattened input tensor;
#X text 292 430 device;
#X text 300 477 add this instance of mha to a specific module;
#X msg 222 510 remove;
#X msg 740 43 1 2 3 4 5 6 7 8 9 10 11 12;
#X text 929 43 flattened input tensor;
#X obj 743 331 print forward;
#X msg 787 251 optimizer adam;
#X obj 743 307 torch.sequential module1;
#X text 231 325 allow bias for the inner linear layers (0 = false \, 1 = true) \, default: true;
#X text 190 148 number of parallel attention heads (each head will focus o in a different part of input: embed/heads) default=2;
#X msg 151 269 batchsize 1;
#X msg 135 191 seqlength 3;
#X text 236 269 batch size (defalt=1);
#X text 161 83 total dimension of the query (dimension of the vector that represents an element (query) in the sequence \, and it must be divisible by heads) default=8;
#X text 439 16 multi-head attention (self-attention);
#X text 321 373 adds a new batch of zeros to the key and value sequences at dim=1 (used in specific cases for numeric stability) default=false, f 41;
#X text 231 191 length of each sequence (define the number of vectors that represent an example (query) \, e.g. \, if embed=4 and seqlenth=3 \, the vector size of each example will be 12) default=1;
#X obj 70 587 route attn_output attn_output_weights;
#X obj 70 695 print out;
#X obj 198 613 print attn_weights;
#X text 673 403 query is a target (what I want to search) and it is represented by a sequence \, key is the label (description) of a query. value is the real content information of each query. in self-attention query=key=value, f 85;
#X text 329 612 the attention wegths tensor is the result of the comparison between query and key \, and represents how relevant one element of the sequence is to the other. shape=[batchsize \, heads \, seqlength(query) \, seqlength(key)];
#X text 140 692 the output tensor is a weighted sum of the attention weights and value \, then the output of each head is concatenated and processed by a fully conected layer. shape=[batchsize \, seqlength \, embed], f 69;
#X obj 70 545 torch.mha -seqlen 1 -batch 3 -embed 4 -heads 2 -bias -weights -addzero -dropout 0.4 -d mps -v;
#X text 772 509 creation arguments: sequence length (default=1) embedding dimension (default=4) \, number of attention heads (default=2) \, weigths (default=false) \, bias (defaul=true \, add zero (default=false) \, dropout (default=0) \, device (default=cpu) \, verbose (default=false), f 73;
#X obj 778 572 torch.mha @seq 1 @btz 3 @e 4 @h 2 @b @w @addzero @drop 0.5 @d cpu @v;
#X connect 0 0 39 0;
#X connect 1 0 39 0;
#X connect 2 0 1 0;
#X connect 3 0 39 0;
#X connect 4 0 3 0;
#X connect 5 0 39 0;
#X connect 6 0 39 0;
#X connect 7 0 39 0;
#X connect 9 0 23 0;
#X connect 10 0 23 0;
#X connect 13 0 23 0;
#X connect 14 0 39 0;
#X connect 18 0 39 0;
#X connect 19 0 23 0;
#X connect 22 0 23 0;
#X connect 23 0 21 0;
#X connect 26 0 39 0;
#X connect 27 0 39 0;
#X connect 33 0 34 0;
#X connect 33 1 35 0;
#X connect 39 0 33 0;
