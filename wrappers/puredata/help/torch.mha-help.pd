#N canvas 838 41 817 1011 12;
#X msg 118 603 embed 4;
#X msg 135 653 heads 2;
#X msg 83 575 1 2 3 4 5 6 7 8 9 10 11 12;
#X msg 153 744 add module1;
#X msg 149 694 seqlength 3;
#X obj 82 839 route attn_output attn_output_weights;
#X obj 82 943 print out;
#X obj 210 880 print attn_weights;
#X text 159 928 the output tensor is a weighted sum of the attention weights and value \, then the output of each head is concatenated and processed by a fully conected layer. shape=[batchsize \, seqlength \, embed], f 69;
#X obj -4 27 cnv 3 800 3 empty empty description 12 12 0 14 #7c7c7c #404040 0;
#X obj -4 85 cnv 3 800 3 empty empty inlets 8 12 0 13 #dcdcdc #000000 0;
#X obj 78 91 cnv 18 3 17 empty empty 0 5 9 0 14 #dcdcdc #9c9c9c 0;
#X obj -4 312 cnv 3 800 3 empty empty outlets 8 12 0 13 #dcdcdc #000000 0;
#X obj 78 320 cnv 18 3 17 empty empty 0 5 9 0 14 #dcdcdc #9c9c9c 0;
#X obj -5 555 cnv 3 800 3 empty empty example 8 12 0 13 #dcdcdc #000000 0;
#X obj -5 -1 cnv 3 800 3 empty empty torch.mha 14 14 1 22 #7c7c7c #404040 0;
#X text 735 8 v.0.1.0;
#X text 652 64 tested on pd-0.56-1;
#X text 100 320 list:;
#X obj -4 368 cnv 3 800 3 empty empty arguments 8 12 0 13 #dcdcdc #000000 0;
#X obj -5 457 cnv 3 800 3 empty empty flags 8 12 0 13 #dcdcdc #000000 0;
#X text 95 533 -verbose: enable verbose logging printings;
#N canvas 294 25 1183 557 (subpatch) 0;
#X obj 385 43 pdcontrol;
#X obj 13 8 inlet;
#X msg 14 40 browse https://github.com/ecrisufmg/contorchionist;
#X connect 1 0 2 0;
#X connect 2 0 0 0;
#X coords 0 -1 1 1 4 2 2 1 1;
#X restore 687 54 pd;
#X msg 687 38 conTorchinist;
#X text 107 94 incoming data: [list];
#X text 275 93 float list of incoming data;
#X text 111 285 device: [symbol];
#X text 282 285 sets the device: cpu \, cuda or mps;
#X text 96 516 -d: sets the device: cpu \, cuda or mps (default=cpu);
#N canvas 294 25 1183 557 (subpatch) 0;
#X obj 385 43 pdcontrol;
#X obj 13 8 inlet;
#X msg 14 40 browse https://arxiv.org/pdf/1706.03762;
#X connect 1 0 2 0;
#X connect 2 0 0 0;
#X coords 0 -1 1 1 4 2 2 1 1;
#X restore 434 58 pd;
#X msg 434 42 Attention Is All You Need;
#X text 104 41 applies an attention layer as described in the;
#X text 109 114 seqlength: [float];
#X text 273 113 length of each sequence (default=1);
#X text 110 133 embed: [float];
#X text 110 158 heads: [float];
#X text 276 131 dimension of the query (default=8);
#X text 277 148 number of parallel attention heads. Note that each head will have dimension equals to embed/heads. (default=2);
#X text 110 182 batchsize: [float];
#X text 276 181 batch size (default=1);
#X text 110 201 bias: [integer];
#X text 278 201 enables learnable bias vector: 0=false \, 1=true;
#X text 110 221 add_zero_attn: [integer];
#X text 291 221 adds a new batch of zeros to the key and value sequences at dim=1, f 65;
#X text 112 244 add: [symbol];
#X text 280 241 adds an instance of torch.linear to a named torch.sequential module, f 67;
#X text 281 262 removes an instance of torch.linear from a named torch.sequential module, f 72;
#X text 109 263 remove: [message];
#X text 145 321 attn_output;
#X text 146 344 attn_output_weights;
#X text 349 861 the attention weights tensor is the result of the comparison between query and key \, and represents how relevant one element of the sequence is to the other. shape=[batchsize \, heads \, seqlength(query) \, seqlength(key)];
#X text 95 375 -seqlen: length of each sequence;
#X text 95 395 -batch: batch size;
#X text 96 415 -heads: number of attention heads;
#X text 97 433 -dropout: dropout probability on attn_output_weights (default=0), f 64;
#N canvas 105 99 883 512 other_messages 0;
#X msg 416 156 device cpu;
#X obj 395 83 tgl 19 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000 0 1;
#X msg 375 52 bias \$1;
#X obj 375 27 tgl 19 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000 0 1;
#X text 498 154 device;
#X msg 428 234 remove;
#X text 437 49 allow bias for the inner linear layers (0 = false \, 1 = true) \, default: true;
#X text 527 97 adds a new batch of zeros to the key and value sequences at dim=1 (used in specific cases for numeric stability) default=false, f 41;
#X msg 395 105 add_zero_attn \$1;
#X text 103 354 query is a target (what I want to search) and it is represented by a sequence \, key is the label (description) of a query. value is the real content information of each query. in self-attention query=key=value, f 85;
#X text 99 440 creation arguments: sequence length (default=1) embedding dimension (default=4) \, number of attention heads (default=2) \, weigths (default=false) \, bias (defaul=true \, add zero (default=false) \, dropout (default=0) \, device (default=cpu) \, verbose (default=false), f 73;
#X obj 208 523 torch.mha @seq 1 @btz 3 @e 4 @h 2 @b @w @addzero @drop 0.5 @d cpu @v;
#X connect 1 0 8 0;
#X connect 3 0 2 0;
#X restore 642 576 pd other_messages;
#X text 280 574 input;
#X text 95 460 -weights: outputs the attn_weigths (default=false);
#X text 96 481 -b: enables learnable bias vector (default=false);
#X text 95 500 -addzero: adds a new batch of zeros to the key and value sequences at dim=1, f 75;
#X text 241 739 add this instance of mha to a specific torch.sequential container. Obs: only self-attention is allowed when added to a torch.sequential container;
#X text 178 601 total dimension of the query (dimension of the vector that represents an element (query) in the sequence \, and it must be divisible by heads);
#X text 201 652 number of parallel attention heads (each head will focus in a different part of input: embed/heads;
#X text 238 686 length of each sequence (define the number of vectors that represent an example (query) \, e.g. \, if embed=4 and seqlenth=3 \, the vector size of each example will be 12), f 75;
#X obj 82 797 torch.mha -seqlen 1 -batch 3 -embed 4 -heads 2 -bias -weights -addzero -dropout 0.4 -d cpu -v;
#X connect 0 0 64 0;
#X connect 1 0 64 0;
#X connect 2 0 64 0;
#X connect 3 0 64 0;
#X connect 4 0 64 0;
#X connect 5 0 6 0;
#X connect 5 1 7 0;
#X connect 23 0 22 0;
#X connect 30 0 29 0;
#X connect 64 0 5 0;
