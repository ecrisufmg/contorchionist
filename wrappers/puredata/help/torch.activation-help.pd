#N canvas 860 42 1343 825 12;
#X msg 438 170 shape 4;
#X msg 487 261 alpha 0.01;
#X msg 316 108 0.945 0.00345 0.00175 0.0836;
#X text 523 108 flattened input tensor;
#X obj 336 612 print;
#X listbox 318 646 30 0 0 0 - - - 0;
#X text 540 649 flattened output tensor;
#X msg 495 317 info;
#X text 536 317 lists all avaliable activation functions;
#X text 581 577 creation arguments: activation (default: sigmoid) \, input tensor shape (default: 4) \, device (default: cpu);
#X msg 493 368 dim 1;
#X text 540 365 set dim parameter for softmax \, logsoftmax \, etc (default: -1);
#X text 581 263 set alpha parameter for leakyrelu \, elu \, etc (default: -1);
#X text 623 221 set the activation function (default: sigmoid);
#X text 500 169 set the input tensor shape (default: 4);
#X msg 500 412 lambda 0.7;
#X msg 461 222 activation softshrink;
#X text 587 411 lambda value for softshrink and hardshrink (default: 0.5);
#X text 344 34 Apply activation function to an input tensor;
#X text 591 457 set the device (default=cpu);
#X msg 501 458 device cuda;
#X obj 319 576 torch.activation softmax 2 2 cpu;
#X connect 0 0 21 0;
#X connect 1 0 21 0;
#X connect 2 0 21 0;
#X connect 7 0 21 0;
#X connect 10 0 21 0;
#X connect 15 0 21 0;
#X connect 16 0 21 0;
#X connect 20 0 21 0;
#X connect 21 0 4 0;
#X connect 21 0 5 0;
