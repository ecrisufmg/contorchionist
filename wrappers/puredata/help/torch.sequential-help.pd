#N canvas 840 52 1014 711 12;
#X text 131 150 destroy the current module and create new;
#X msg 81 183 info;
#X msg 94 222 clear;
#X text 149 223 reset the current module (destroy and re-create a new module with the same name);
#X msg 46 150 set module1;
#X msg 118 306 optimizer adam;
#X text 235 305 sets the optimizer and learning rate (default: adam 0.001);
#X text 124 182 lists all activated module;
#X text 268 496 train the module through the number of epochs specified (default: 100);
#X msg 191 499 train 300;
#X msg 148 385 target \$1;
#X obj 148 362 tgl 19 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000 0 1;
#X msg 108 266 device cpu;
#X text 199 268 sets the device: cpu \, cuda \, mps (default: cpu);
#X text 365 423 load a dataset from .txt file;
#X msg 129 336 loss mse;
#X text 196 335 set the loss function;
#X text 221 375 sets target=1 \, if the dataset has target/label or sets target=0 \, if the dataset has no target/label (default=0);
#X text 23 23 pdtorch.sequential allows to create and train models (with specific names) that can be built by a sequence of modules added by instances of other objects (like pdtorch.mha and pdtorch.activation). When a pdtorch.sequencial model \, receives an input \, this input will pass through all layers contained in that model \, according to the order in which it was added.;
#X obj 808 312 torch.linear;
#X msg 807 289 add module1;
#X text 363 536 initialize the layers weights with a specified method;
#X msg 197 540 init he_uniform relu 0;
#X text 363 551 he uniform: nonlinearity negative slope (default: relu 0) he normal: nonlinearity negative slope (default: leaky_relu 0.2) xavier uniform: gain (defalt: 1) uniform: low high (default: 0 1) normal mean: std (default: 0 0.02) constant: value (default: 0), f 70;
#X obj 606 13 torch.sequential module2 @opt adam @loss bce @xaviernormal @g 1 @lr 0.0001 @target @seed 42 @device mps @v, f 45;
#X obj 50 659 torch.sequential module1 -opt adam -loss mse -lr 0.0001 -init normal mean 0 std 0.01 -target -d mps -v;
#X text 604 60 creation arguments and flags: module name \; optimizer: adam \, adamw \, adagrad \, lbfgs \, rmsprop \, sgd (default=sgd) \; loss function: mse \, l1 \, smooth l1 \, cross entropy \, nll \, binary cross entropy \, binary cross entropy with logits \, kl divergence \, hinge embendding \, multi margin \, multi label margin \, multi label soft margin \, soft margin (default=mse) \; weight initialization: he uniform \, he normal \, xavier uniform \, xavier normal \, uniform \, normal \, constant (default=uniform-low=0 \, high=1) \; random state (default=-1) \; learning rate (default=1e-03) \; target (default=false) \; outloss (default=false) \; device (default=cpu) \; verbose (default=false), f 57;
#X msg 161 422 load data/modelo_test.txt;
#X text 236 461 set a torch.ls2tensor tensor to use it as training dataset;
#X msg 171 462 dataset;
#X connect 1 0 25 0;
#X connect 2 0 25 0;
#X connect 4 0 25 0;
#X connect 5 0 25 0;
#X connect 9 0 25 0;
#X connect 10 0 25 0;
#X connect 11 0 10 0;
#X connect 12 0 25 0;
#X connect 15 0 25 0;
#X connect 20 0 19 0;
#X connect 22 0 25 0;
#X connect 27 0 25 0;
#X connect 29 0 25 0;
